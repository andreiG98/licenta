{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"model.ipynb","provenance":[{"file_id":"1GynPfJ43suxzrObHy7r_bMSvigpwkBOl","timestamp":1586095535370}],"collapsed_sections":[],"authorship_tag":"ABX9TyNoCruevQZ82a0q1TfIupeB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"uGy8aRmBwuz9","colab_type":"code","outputId":"b8af4c00-d50e-4bfd-cc2a-d97492cfec3a","executionInfo":{"status":"ok","timestamp":1591704474346,"user_tz":-180,"elapsed":1289,"user":{"displayName":"ANDREI-CRISTIAN GIDEA","photoUrl":"","userId":"01539519734302621580"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')\n","\n","%cd 'gdrive/My Drive/licenta/car_model_classification'"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive/; to attempt to forcibly remount, call drive.mount(\"/content/gdrive/\", force_remount=True).\n","[Errno 2] No such file or directory: 'gdrive/My Drive/licenta/car_model_classification'\n","/content/gdrive/My Drive/licenta/car_model_classification\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5raE3kBY7viT","colab_type":"code","outputId":"ed668c81-9e45-4bdf-8e43-7462408be05e","executionInfo":{"status":"ok","timestamp":1591704476080,"user_tz":-180,"elapsed":3007,"user":{"displayName":"ANDREI-CRISTIAN GIDEA","photoUrl":"","userId":"01539519734302621580"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["import argparse\n","import json\n","import os\n","import time\n","\n","import pandas as pd\n","import torch\n","import torch.optim as optim\n","import torch.nn as nn\n","from torch.optim import lr_scheduler\n","from torch.utils.tensorboard import SummaryWriter\n","import torchvision\n","\n","from model.se_resnet import se_resnet50, SELayer\n","from model.se_linear_resnet import se_linear_resnet50, se_linear_resnet34, SELayer\n","from model.cbam_resnet import cbam_resnet50\n","from model.cbam_linear_resnet import cbam_linear_resnet50, cbam_linear_resnet34, CBAMBlock\n","from model.cbam_resnet_official import cbam_official_resnet50\n","# from model.se_vgg import se_vgg19_bn\n","from datasets import get_train_valid_loader\n","\n","print(\"PyTorch Version: \",torch.__version__)\n","print(\"Torchvision Version: \",torchvision.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["PyTorch Version:  1.5.0+cu101\n","Torchvision Version:  0.6.0+cu101\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zclza4G1GiBr","colab_type":"code","outputId":"dfafac6b-5688-480e-84c4-0c6f1b42850f","executionInfo":{"status":"ok","timestamp":1591704480252,"user_tz":-180,"elapsed":7165,"user":{"displayName":"ANDREI-CRISTIAN GIDEA","photoUrl":"","userId":"01539519734302621580"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["%pip install torchsummary"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchsummary in /usr/local/lib/python3.6/dist-packages (1.5.1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"AHIHewQGM5QW","colab_type":"code","outputId":"c675ef57-6309-4a2c-db5d-79f7a2bb11d7","executionInfo":{"status":"ok","timestamp":1591704483031,"user_tz":-180,"elapsed":9931,"user":{"displayName":"ANDREI-CRISTIAN GIDEA","photoUrl":"","userId":"01539519734302621580"}},"colab":{"base_uri":"https://localhost:8080/","height":289}},"source":["!nvidia-smi"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Tue Jun  9 12:08:00 2020       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 440.82       Driver Version: 418.67       CUDA Version: 10.1     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|===============================+======================+======================|\n","|   0  Tesla K80           Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   54C    P0    56W / 149W |   4017MiB / 11441MiB |      0%      Default |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                       GPU Memory |\n","|  GPU       PID   Type   Process name                             Usage      |\n","|=============================================================================|\n","+-----------------------------------------------------------------------------+\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0iG8LR_e9BVJ","colab_type":"code","colab":{}},"source":["def train_model(model, dataloaders, criterion, optimizer, lr_scheduler, num_epochs, exp_dir, device, writer):\n","    since = time.time()\n","\n","    best_acc = 0.0\n","\n","    res = []\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch + 1, num_epochs))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            runcount = 0\n","            i = 0\n","\n","            # Iterate over data.\n","            for inputs, labels in dataloaders[phase]:\n","                inputs = inputs.to(device)\n","                labels = labels.to(device)\n","\n","                # zero the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    # Get model outputs and calculate loss\n","                    \n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, labels)\n","\n","                    _, preds = torch.max(outputs, 1)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                batch_size = inputs.size(0)\n","                running_loss += loss.item() * batch_size\n","                running_corrects += torch.sum(preds == labels.data)\n","\n","                lr = get_lr(optimizer)\n","                runcount += batch_size\n","                i += 1\n","\n","                print(f'{phase} [{i}/{len(dataloaders[phase])}]: '\n","                    f'Loss: {running_loss / runcount:.4f} '\n","                    f'Acc: {(running_corrects.double() / runcount * 100):.2f}% '\n","                    f'Learning Rate: {lr}')\n","\n","            epoch_loss = running_loss / runcount\n","            epoch_acc = running_corrects.double() / runcount * 100\n","\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                \n","                torch.save({\n","                    'model': model.state_dict(),\n","                    'optimizer': optimizer.state_dict(),\n","                    'lr_scheduler': lr_scheduler.state_dict(),\n","                    'epoch': epoch},\n","                    os.path.join(exp_dir, 'best.pth'))\n","                \n","            if phase == 'train':\n","                trainres = {\n","                    'train_loss': epoch_loss,\n","                    'train_acc': epoch_acc.item(),\n","                }\n","\n","                # Record loss / acc into the writer\n","                writer.add_scalar('Train/Loss', epoch_loss, epoch)\n","                writer.add_scalar('Train/Accuracy', epoch_acc, epoch)\n","                writer.flush()\n","\n","            if phase == 'val':\n","                valres = {\n","                    'val_loss': epoch_loss,\n","                    'val_acc': epoch_acc.item(),\n","                }\n","\n","                trainres.update(valres)\n","                res.append(trainres)\n","\n","                # Record loss / acc into the writer\n","                writer.add_scalar('Val/Loss', epoch_loss, epoch)\n","                writer.add_scalar('Val/Accuracy', epoch_acc, epoch)\n","                writer.flush()\n","\n","                try:\n","                    lr_scheduler.step()\n","                except:\n","                    lr_scheduler.step(epoch_loss)\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:4f}'.format(best_acc))\n","\n","    res = pd.DataFrame(res)\n","    res.to_csv(exp_dir + '/history.csv')\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gl83NBoKEmtY","colab_type":"code","colab":{}},"source":["import math\n","import matplotlib.pyplot as plt\n","\n","def find_lr(model, loss_fn, optimizer, train_loader, device, init_value=1e-8, final_value=10.0):\n","    number_in_epoch = len(train_loader) - 1\n","    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n","    lr = init_value\n","    optimizer.param_groups[0][\"lr\"] = lr\n","    best_loss = 0.0\n","    batch_num = 0\n","    losses = []\n","    log_lrs = []\n","    \n","    for data in train_loader:\n","        batch_num += 1\n","        inputs, labels = data\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        loss = loss_fn(outputs, labels)\n","\n","        # Crash out if loss explodes\n","\n","        if batch_num > 1 and loss > 4 * best_loss:\n","            return log_lrs[10:-5], losses[10:-5]\n","\n","        # Record the best loss\n","\n","        if loss < best_loss or batch_num == 1:\n","            best_loss = loss\n","\n","        # Store the values\n","\n","        losses.append(loss)\n","        log_lrs.append(math.log10(lr))\n","\n","        # Do the backward pass and optimize\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        # Update the lr for the next step and store\n","\n","        lr *= update_step\n","        optimizer.param_groups[0][\"lr\"] = lr\n","        \n","    return log_lrs[10:-5], losses[10:-5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2JHrPCS09GAv","colab_type":"code","colab":{}},"source":["def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"P3YFhQCLI9Sl","colab_type":"code","colab":{}},"source":["class NetworkV1(nn.Module):\n","    def __init__(self, feature_extractor, num_classes, feature_extract):\n","        super().__init__()\n","\n","        if hasattr(feature_extractor, 'fc'):\n","            self.feature_extractor = feature_extractor\n","            in_features = feature_extractor.fc.in_features\n","            # self.feature_extractor.fc = nn.Sequential(\n","            #     nn.Dropout(),\n","            #     nn.Linear(in_features=in_features, out_features=512),\n","            #     nn.ReLU(),\n","            #     nn.Dropout(),\n","            #     nn.Linear(in_features=512, out_features=256),\n","            #     nn.ReLU(),\n","            #     nn.Dropout(),\n","            #     nn.Linear(in_features=256, out_features=num_classes)\n","            # )\n","            self.feature_extractor.fc = nn.Linear(in_features=in_features, out_features=num_classes)\n","        elif isinstance(feature_extractor.classifier, nn.Linear): # densenet121\n","            in_features = self.feature_extractor.classifier.in_features\n","            self.feature_extractor.classifier = nn.Sequential(\n","                                        nn.Linear(in_features, num_classes),\n","                                        nn.Sigmoid())\n","        else: # mobilenetv2 / vgg19\n","            self.feature_extractor = feature_extractor.features\n","            self.avgpool = feature_extractor.avgpool\n","            self.classifier = feature_extractor.classifier\n","            if feature_extract:\n","                self.feature_extractor.eval()\n","            in_features = self.classifier[-1].in_features\n","            self.classifier[-1] = nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n","\n","    def forward(self, x):\n","        # x = self.feature_extractor(x)\n","        # x = self.avgpool(x)\n","        # x = torch.flatten(x, 1)\n","        # x = self.classifier(x)\n","        \n","        # return x\n","\n","        fc = self.feature_extractor(x)\n","\n","        return fc"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"gPzrq63QDHPc","colab_type":"code","colab":{}},"source":["class NetworkGradCam(nn.Module):\n","    def __init__(self, feature_extractor, num_classes, feature_extract):\n","        super().__init__()\n","\n","        # self.feature_extractor = feature_extractor\n","\n","        # gradient placeholder\n","        self.gradient = None\n","\n","        if hasattr(feature_extractor, 'fc'): # resnets\n","            # isolate the feature blocks\n","            self.features = nn.Sequential(feature_extractor.conv1,\n","                                          feature_extractor.bn1,\n","                                          nn.ReLU(),\n","                                          nn.MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False),\n","                                          feature_extractor.layer1, \n","                                          feature_extractor.layer2, \n","                                          feature_extractor.layer3, \n","                                          feature_extractor.layer4)\n","            # average pooling layer\n","            self.avgpool = feature_extractor.avgpool\n","\n","            in_features = feature_extractor.fc.in_features\n","            # if feature_extract:\n","            #     self.classifier = nn.Sequential(\n","            #         nn.Linear(in_features=in_features, out_features=512),\n","            #         nn.BatchNorm1d(num_features=512),\n","            #         nn.ReLU(),\n","            #         # nn.Dropout(),\n","            #         nn.Linear(in_features=512, out_features=256),\n","            #         nn.BatchNorm1d(num_features=256),\n","            #         nn.ReLU(),\n","            #         # nn.Dropout(),\n","            #         nn.Linear(in_features=256, out_features=num_classes)\n","            #     )\n","            # else:\n","            #     # classifier\n","            #     self.classifier = nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n","            \n","            # classifier\n","            self.classifier = nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n","\n","        # elif hasattr(feature_extractor, 'last_linear'): # se_resnets\n","        #     # isolate the feature blocks\n","        #     self.features = nn.Sequential(feature_extractor.layer0,\n","        #                                   feature_extractor.layer1, \n","        #                                   feature_extractor.layer2, \n","        #                                   feature_extractor.layer3, \n","        #                                   feature_extractor.layer4)\n","            \n","        #     self.avgpool = feature_extractor.avg_pool\n","\n","        #     in_features = feature_extractor.last_linear.in_features\n","        #     if feature_extract:\n","        #         self.classifier = nn.Sequential(\n","        #             nn.Linear(in_features=in_features, out_features=512),\n","        #             nn.ReLU(),\n","        #             nn.Dropout(),\n","        #             nn.Linear(in_features=512, out_features=256),\n","        #             nn.ReLU(),\n","        #             nn.Dropout(),\n","        #             nn.Linear(in_features=256, out_features=num_classes)\n","        #         )\n","        #     else:\n","        #         # classifier\n","        #         self.classifier = nn.Linear(in_features=in_features, out_features=num_classes, bias=True)\n","\n","        else: # vgg\n","            self.features = feature_extractor.features\n","            if feature_extract:\n","                self.features.eval()\n","\n","            # average pooling layer\n","            self.avgpool = feature_extractor.avgpool\n","\n","            in_features_first = feature_extractor.classifier[0].in_features\n","            in_features_last = feature_extractor.classifier[-1].in_features\n","            self.classifier = nn.Sequential(\n","                    nn.Linear(in_features=in_features_first, out_features=in_features_last, bias=True),\n","                    nn.ReLU(),\n","                    nn.Dropout(),\n","                    nn.Linear(in_features=in_features_last, out_features=in_features_last, bias=True),\n","                    nn.ReLU(),\n","                    nn.Dropout(),\n","                    nn.Linear(in_features=in_features_last, out_features=num_classes, bias=True)\n","                )\n","            \n","    # hook for the gradients\n","    def activations_hook(self, grad):\n","        self.gradient = grad\n","    \n","    def get_gradient(self):\n","        return self.gradient\n","    \n","    def get_activations(self, x):\n","        return self.features(x)\n","\n","    def forward(self, x):\n","        # extract the features\n","        x = self.features(x)\n","        \n","        # # register the hook\n","        # h = x.register_hook(self.activations_hook)\n","        \n","        # complete the forward pass\n","        x = self.avgpool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.classifier(x)\n","        \n","        return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SoAKYy1EQ8W6","colab_type":"code","colab":{}},"source":["# from torchsummary import summary\n","# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# net = NetworkGradCam(cbam_official_resnet50(pretrained=False), 157, False).to(device)\n","\n","# summary(net, (3, 224, 224))\n","\n","# # x = torch.randn(size=(1, 3, 448, 448)).to(device)\n","\n","# # pred = net(x)\n","\n","# print(net)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFZPYyPKJD_f","colab_type":"code","colab":{}},"source":["def construct_model(config, num_classes):\n","    if config['arch'] == 'resnext50':\n","        feature_extractor = torchvision.models.resnext50_32x4d(pretrained=config['pretrained'])\n","    elif config['arch'] == 'resnet18':\n","        feature_extractor = torchvision.models.resnet18(pretrained=config['pretrained'])\n","    elif config['arch'] == 'resnet34':\n","        feature_extractor = torchvision.models.resnet34(pretrained=config['pretrained'])\n","    elif config['arch'] == 'resnet50':\n","        feature_extractor = torchvision.models.resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'mobilenetv2':\n","        feature_extractor = torchvision.models.mobilenet_v2(pretrained=config['pretrained'])\n","    elif config['arch'] == 'vgg19':\n","        feature_extractor = torchvision.models.vgg19(pretrained=config['pretrained'])\n","    elif config['arch'] == 'vgg19_bn':\n","        feature_extractor = torchvision.models.vgg19_bn(pretrained=config['pretrained'])\n","    elif config['arch'] == 'densenet121':\n","        feature_extractor = torchvision.models.densenet121(pretrained=config['pretrained'])\n","    elif config['arch'] == 'se_resnet50':\n","        feature_extractor = se_resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'se_linear_resnet34':\n","        feature_extractor = se_linear_resnet34(pretrained=config['pretrained'])\n","    elif config['arch'] == 'se_linear_resnet50':\n","        feature_extractor = se_linear_resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'cbam_resnet50':\n","        feature_extractor = cbam_resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'cbam_linear_resnet34':\n","        feature_extractor = cbam_linear_resnet34(pretrained=config['pretrained'])\n","    elif config['arch'] == 'cbam_linear_resnet50':\n","        feature_extractor = cbam_linear_resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'cbam_official_resnet50':\n","        feature_extractor = cbam_official_resnet50(pretrained=config['pretrained'])\n","    elif config['arch'] == 'se_vgg19_bn':\n","        feature_extractor = se_vgg19_bn(pretrained=config['pretrained'])\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","\n","    \n","    if config['feature_extract']:\n","        if config['arch'] == 'resnet50':\n","            child_counter = 0\n","            for child in feature_extractor.children():\n","                if child_counter < 6 and not isinstance(child, nn.BatchNorm2d):\n","                  print(\"child was frozen\".format(child_counter))\n","                  for name, param in child.named_parameters():\n","                      print(name)\n","                      if(\"bn\" not in name):\n","                          param.requires_grad = False\n","                else:\n","                    print(\"child was not frozen\".format(child_counter))\n","                child_counter += 1\n","        else:\n","          child_counter = 0\n","          for child in feature_extractor.children():\n","              if child_counter < 4 and not isinstance(child, nn.BatchNorm2d):\n","                  print(\"child {} was frozen\".format(child_counter))\n","                  print(child)\n","                  for name, param in child.named_parameters():\n","                      param.requires_grad = False\n","              else:\n","                for children_of_child in child.children():\n","                    children_of_child_counter = 0\n","                    for child_child in children_of_child.children():\n","                        child_child_counter = 0\n","                        if isinstance(child_child, SELayer) or isinstance(child_child, CBAMBlock):\n","                            print(\"child {} of child {} of child {} was not frozen\".format(child_child_counter, children_of_child_counter, child_counter))\n","                            print(child_child)\n","                        else:\n","                            if child_counter < 6 and not isinstance(child_child, nn.BatchNorm2d):\n","                                print(\"child {} of child {} of child {} was frozen\".format(child_child_counter, children_of_child_counter, child_counter))\n","                                print(child_child)\n","                                for name, param in child_child.named_parameters():\n","                                    print(name)\n","                                    if(\"bn\" not in name):\n","                                        param.requires_grad = False\n","                            else:\n","                                print(\"child {} of child {} of child {} was not frozen\".format(child_child_counter, children_of_child_counter, child_counter))\n","                                print(child_child)\n","\n","                        child_child_counter += 1\n","                        children_of_child_counter += 1\n","              child_counter += 1\n","            \n","    # if config['version'] == '1':\n","    #     model = NetworkV1(feature_extractor, num_classes, config['feature_extract'])\n","    if config['grad_cam']:\n","        model = NetworkGradCam(feature_extractor, num_classes, config['feature_extract'])\n","    \n","    # model = feature_extractor\n","\n","    return model"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"LuaieZlJ9XhX","colab_type":"code","colab":{}},"source":["def get_lr(optimizer):\n","    for param_group in optimizer.param_groups:\n","        return param_group['lr']"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QSZURXt8HAoL","colab_type":"code","colab":{}},"source":["def get_exp_dir(config):\n","\n","    if config['cifar10']:\n","        os.makedirs('logs/cifar10', exist_ok=True)\n","        exp_dir = f'logs/cifar10/{config[\"arch\"]}_{config[\"imgsize\"][0]}_{config[\"epochs\"]}'\n","    else:\n","        exp_dir = f'logs/{config[\"arch\"]}_{config[\"imgsize\"][0]}_{config[\"epochs\"]}'\n","\n","    if config['finetune']:\n","        exp_dir += '_finetune'\n","\n","    if config['feature_extract']:\n","        exp_dir += '_feature_extract'\n","      \n","    if config['grad_cam']:\n","        exp_dir += '_grad_cam'\n","      \n","    if config['pretrained'] is False:\n","        exp_dir += '_no_pretrained'\n","\n","    os.makedirs(exp_dir, exist_ok=True)\n","\n","    exps = [d for d in os.listdir(exp_dir) if os.path.isdir(os.path.join(exp_dir, d))]\n","    files = set(map(int, exps))\n","    if len(files):\n","        exp_id = min(set(range(1, max(files) + 2)) - files)\n","    else:\n","        exp_id = 1\n","\n","    exp_dir = os.path.join(exp_dir, str(exp_id))\n","    os.makedirs(exp_dir, exist_ok=True)\n","\n","    json.dump(config, open(exp_dir + '/config.json', 'w'))\n","\n","    return exp_dir\n","\n","def load_weight(model, optimizer, lr_scheduler, path):\n","    sd = torch.load(path)\n","    model.load_state_dict(sd['model'])\n","    optimizer.load_state_dict(sd['optimizer'])\n","    lr_scheduler.load_state_dict(sd['lr_scheduler'])\n","    epoch = sd['epoch']\n","\n","    print('Loaded model from epoch %d\\n' % (epoch))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"V3LzfyV0Gqmd","colab_type":"code","colab":{}},"source":["# %pip install torch-lr-finder\n","# %pip install torch-lr-finder -v --global-option=\"amp\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"k-8iI6bG-rES","colab_type":"code","colab":{}},"source":["from torchsummary import summary\n","\n","def main(args):\n","    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","    config = {\n","        'batch_size': args.batch_size,\n","        'optimizer': args.optim,\n","        'lr': args.lr,\n","        'weight_decay': args.weight_decay,\n","        'momentum': args.momentum,\n","        'epochs': args.epochs,\n","        'imgsize': (args.imgsize, args.imgsize),\n","        'arch': args.arch,\n","        'finetune': args.finetune,\n","        'path': args.path,\n","        'feature_extract': args.feature_extract,\n","        'version': '1',\n","        'grad_cam': args.grad_cam,\n","        'pretrained': args.pretrained,\n","        'cifar10': args.cifar10\n","    }\n","\n","    dataloaders_dict = get_train_valid_loader(config)\n","\n","    class_names = dataloaders_dict['train'].dataset.classes\n","    num_classes = len(class_names)\n","\n","    model = construct_model(config, num_classes=num_classes)\n","    model = model.to(device)\n","    summary(model, (3, config['imgsize'][0], config['imgsize'][1]))\n","\n","    # print(model)\n","\n","    # Gather the parameters to be optimized/updated in this run. If we are\n","    #  finetuning we will be updating all parameters. However, if we are\n","    #  doing feature extract method, we will only update the parameters\n","    #  that we have just initialized, i.e. the parameters with requires_grad\n","    #  is True.\n","    params_to_update = model.parameters()\n","    print(\"Params to learn:\")\n","    if config['feature_extract']:\n","        params_to_update = []\n","        for name,param in model.named_parameters():\n","            if param.requires_grad == True:\n","                params_to_update.append(param)\n","                print(\"\\t\",name)\n","\n","    optimizer_ft = optim.SGD(params_to_update,\n","                            lr=config['lr'],\n","                            momentum=config['momentum'],\n","                            weight_decay=config['weight_decay'])\n","    if config['optimizer'].lower == 'adam':\n","        optimizer_ft = optim.Adam(params_to_update,\n","                                  lr=config['lr'],\n","                                  weight_decay=config['weight_decay'])\n","\n","    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer_ft,\n","                                                        factor = 0.1,\n","                                                        patience = 5,\n","                                                        mode = 'min')\n","\n","    if config['finetune']:\n","        load_weight(model, optimizer_ft, lr_scheduler, config['path'])\n","\n","    # Setup the loss fxn\n","    criterion = nn.CrossEntropyLoss()\n","\n","    # # %pip install torch-lr-finder -v --global-option=\"amp\"\n","\n","    # # from torch_lr_finder import LRFinder\n","    \n","    # # lr_finder = LRFinder(model, optimizer_ft, criterion, device='cuda')\n","    # # lr_finder.range_test(dataloaders_dict['train'], val_loader=dataloaders_dict['val'], end_lr=10, num_iter=100, step_mode='exp')\n","    # # lr_finder.plot()\n","\n","    # # logs, losses = find_lr(model, criterion, optimizer_ft, dataloaders_dict['train'], device)\n","    # # plt.plot(logs, losses)\n","\n","    # exp_dir = get_exp_dir(config)\n","\n","    # PATH_to_log_dir = exp_dir + '/runs/'\n","    # # Declare Tensorboard writer\n","    # writer = SummaryWriter(PATH_to_log_dir)\n","    # print('Tensorboard is recording into folder: ' + PATH_to_log_dir + '\\n')\n","\n","    # # Train and evaluate\n","    # model = train_model(model, dataloaders_dict, criterion, optimizer_ft, lr_scheduler, config['epochs'], exp_dir, device, writer)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZT0S7Eq-aAT","colab_type":"code","outputId":"23f9afc4-f846-4cc2-a3df-e9232a6339fd","executionInfo":{"status":"ok","timestamp":1591704636309,"user_tz":-180,"elapsed":2133,"user":{"displayName":"ANDREI-CRISTIAN GIDEA","photoUrl":"","userId":"01539519734302621580"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["parser = argparse.ArgumentParser(description='Training and finetuning script for Car Model classification')\n","\n","# training arg\n","parser.add_argument('--batch-size', default=32, type=int,\n","                    help='training batch size (default: 32)')\n","parser.add_argument('--epochs', default=40, type=int,\n","                    help='training epochs (default: 40)')\n","parser.add_argument('--arch', default='resnet34', choices=['resnext50',\n","                                                            'resnet18',\n","                                                            'resnet34',\n","                                                            'resnet50',\n","                                                            'vgg19',\n","                                                            'vgg19_bn',\n","                                                            'se_resnet50',\n","                                                            'se_linear_resnet34',\n","                                                            'se_linear_resnet50',\n","                                                            'cbam_resnet50',\n","                                                            'cbam_linear_resnet34',\n","                                                            'cbam_linear_resnet50',\n","                                                            'cbam_official_resnet50',\n","                                                            'se_vgg19_bn'],\n","                    help='Architecture (default: resnet34)')\n","parser.add_argument('--imgsize', default=224, type=int,\n","                    help='Input image size')\n","parser.add_argument('--finetune', default=False, action='store_true',\n","                    help='whether to finetune from 400x400 to 224x224 (default: False) or to resume training')\n","parser.add_argument('--path', default=None,\n","                    help='required if it is a finetune task (default: None)')\n","parser.add_argument('--feature-extract', default=False, action='store_true',\n","                    help='whether to feature extract (default: False)')\n","parser.add_argument('--grad-cam', default=False, action='store_true',\n","                    help='grad cam network or not')\n","parser.add_argument('--pretrained', default=False, action='store_true',\n","                    help='Imagenet pretrained weights')\n","\n","# optimizer arg\n","parser.add_argument('--optim', default='SGD', type=str,\n","                    help='Optimizer (default: SGD)')\n","parser.add_argument('--lr', default=0.01, type=float,\n","                    help='Optimizer learning rate (default: 0.01)')\n","parser.add_argument('--weight-decay', default=1e-4, type=float,\n","                    help='Optimizer weight decay (default: 0.0001)')\n","parser.add_argument('--momentum', default=0.9, type=float,\n","                    help='SGD momentum (default: 0.9)')\n","\n","parser.add_argument('--cifar10', default=False, action='store_true')\n","\n","# args = parser.parse_args()\n","\n","args = parser.parse_args(args=['--arch', 'resnet34', '--batch-size', '64', '--lr', '0.01', '--epochs', '50', '--imgsize', '224', '--grad-cam', '--pretrained',\n","                               '--feature-extract'])\n","                              #  '--cifar10'])\n","                              #  '--finetune', '--path', 'logs/cbam_linear_resnet34_224_25_grad_cam/1/best.pth'])\n","\n","if args.finetune and args.path is None:\n","    parser.error('--finetune requires --path')\n","\n","main(args)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Initializing datasets and dataloaders for train and validation...\n","child 0 was frozen\n","Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","child 2 was frozen\n","ReLU(inplace=True)\n","child 3 was frozen\n","MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","child 0 of child 0 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 4 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 4 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 4 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 4 was frozen\n","Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 4 was not frozen\n","BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 5 was frozen\n","Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 5 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 5 of child 5 was frozen\n","Sequential(\n","  (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","  (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",")\n","0.weight\n","1.weight\n","1.bias\n","child 0 of child 0 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 5 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 5 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 1 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 5 was frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 5 was frozen\n","Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","weight\n","child 0 of child 4 of child 5 was not frozen\n","BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 5 of child 6 was not frozen\n","Sequential(\n","  (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","  (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",")\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 6 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 6 was not frozen\n","Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 6 was not frozen\n","BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 7 was not frozen\n","Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 7 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 7 was not frozen\n","Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 5 of child 7 was not frozen\n","Sequential(\n","  (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","  (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",")\n","child 0 of child 0 of child 7 was not frozen\n","Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 7 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 7 was not frozen\n","Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 0 of child 7 was not frozen\n","Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 1 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","child 0 of child 2 of child 7 was not frozen\n","ReLU(inplace=True)\n","child 0 of child 3 of child 7 was not frozen\n","Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","child 0 of child 4 of child 7 was not frozen\n","BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","       BasicBlock-11           [-1, 64, 56, 56]               0\n","           Conv2d-12           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-13           [-1, 64, 56, 56]             128\n","             ReLU-14           [-1, 64, 56, 56]               0\n","           Conv2d-15           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-16           [-1, 64, 56, 56]             128\n","             ReLU-17           [-1, 64, 56, 56]               0\n","       BasicBlock-18           [-1, 64, 56, 56]               0\n","           Conv2d-19           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-20           [-1, 64, 56, 56]             128\n","             ReLU-21           [-1, 64, 56, 56]               0\n","           Conv2d-22           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-23           [-1, 64, 56, 56]             128\n","             ReLU-24           [-1, 64, 56, 56]               0\n","       BasicBlock-25           [-1, 64, 56, 56]               0\n","           Conv2d-26          [-1, 128, 28, 28]          73,728\n","      BatchNorm2d-27          [-1, 128, 28, 28]             256\n","             ReLU-28          [-1, 128, 28, 28]               0\n","           Conv2d-29          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-30          [-1, 128, 28, 28]             256\n","           Conv2d-31          [-1, 128, 28, 28]           8,192\n","      BatchNorm2d-32          [-1, 128, 28, 28]             256\n","             ReLU-33          [-1, 128, 28, 28]               0\n","       BasicBlock-34          [-1, 128, 28, 28]               0\n","           Conv2d-35          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-36          [-1, 128, 28, 28]             256\n","             ReLU-37          [-1, 128, 28, 28]               0\n","           Conv2d-38          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-39          [-1, 128, 28, 28]             256\n","             ReLU-40          [-1, 128, 28, 28]               0\n","       BasicBlock-41          [-1, 128, 28, 28]               0\n","           Conv2d-42          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-43          [-1, 128, 28, 28]             256\n","             ReLU-44          [-1, 128, 28, 28]               0\n","           Conv2d-45          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-46          [-1, 128, 28, 28]             256\n","             ReLU-47          [-1, 128, 28, 28]               0\n","       BasicBlock-48          [-1, 128, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","       BasicBlock-55          [-1, 128, 28, 28]               0\n","           Conv2d-56          [-1, 256, 14, 14]         294,912\n","      BatchNorm2d-57          [-1, 256, 14, 14]             512\n","             ReLU-58          [-1, 256, 14, 14]               0\n","           Conv2d-59          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-60          [-1, 256, 14, 14]             512\n","           Conv2d-61          [-1, 256, 14, 14]          32,768\n","      BatchNorm2d-62          [-1, 256, 14, 14]             512\n","             ReLU-63          [-1, 256, 14, 14]               0\n","       BasicBlock-64          [-1, 256, 14, 14]               0\n","           Conv2d-65          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-66          [-1, 256, 14, 14]             512\n","             ReLU-67          [-1, 256, 14, 14]               0\n","           Conv2d-68          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-69          [-1, 256, 14, 14]             512\n","             ReLU-70          [-1, 256, 14, 14]               0\n","       BasicBlock-71          [-1, 256, 14, 14]               0\n","           Conv2d-72          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-73          [-1, 256, 14, 14]             512\n","             ReLU-74          [-1, 256, 14, 14]               0\n","           Conv2d-75          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-76          [-1, 256, 14, 14]             512\n","             ReLU-77          [-1, 256, 14, 14]               0\n","       BasicBlock-78          [-1, 256, 14, 14]               0\n","           Conv2d-79          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-80          [-1, 256, 14, 14]             512\n","             ReLU-81          [-1, 256, 14, 14]               0\n","           Conv2d-82          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","             ReLU-84          [-1, 256, 14, 14]               0\n","       BasicBlock-85          [-1, 256, 14, 14]               0\n","           Conv2d-86          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-87          [-1, 256, 14, 14]             512\n","             ReLU-88          [-1, 256, 14, 14]               0\n","           Conv2d-89          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-90          [-1, 256, 14, 14]             512\n","             ReLU-91          [-1, 256, 14, 14]               0\n","       BasicBlock-92          [-1, 256, 14, 14]               0\n","           Conv2d-93          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-94          [-1, 256, 14, 14]             512\n","             ReLU-95          [-1, 256, 14, 14]               0\n","           Conv2d-96          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-97          [-1, 256, 14, 14]             512\n","             ReLU-98          [-1, 256, 14, 14]               0\n","       BasicBlock-99          [-1, 256, 14, 14]               0\n","          Conv2d-100            [-1, 512, 7, 7]       1,179,648\n","     BatchNorm2d-101            [-1, 512, 7, 7]           1,024\n","            ReLU-102            [-1, 512, 7, 7]               0\n","          Conv2d-103            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-104            [-1, 512, 7, 7]           1,024\n","          Conv2d-105            [-1, 512, 7, 7]         131,072\n","     BatchNorm2d-106            [-1, 512, 7, 7]           1,024\n","            ReLU-107            [-1, 512, 7, 7]               0\n","      BasicBlock-108            [-1, 512, 7, 7]               0\n","          Conv2d-109            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-110            [-1, 512, 7, 7]           1,024\n","            ReLU-111            [-1, 512, 7, 7]               0\n","          Conv2d-112            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-113            [-1, 512, 7, 7]           1,024\n","            ReLU-114            [-1, 512, 7, 7]               0\n","      BasicBlock-115            [-1, 512, 7, 7]               0\n","          Conv2d-116            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-117            [-1, 512, 7, 7]           1,024\n","            ReLU-118            [-1, 512, 7, 7]               0\n","          Conv2d-119            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-120            [-1, 512, 7, 7]           1,024\n","            ReLU-121            [-1, 512, 7, 7]               0\n","      BasicBlock-122            [-1, 512, 7, 7]               0\n","AdaptiveAvgPool2d-123            [-1, 512, 1, 1]               0\n","          Linear-124                  [-1, 157]          80,541\n","================================================================\n","Total params: 21,365,213\n","Trainable params: 20,020,253\n","Non-trainable params: 1,344,960\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 96.28\n","Params size (MB): 81.50\n","Estimated Total Size (MB): 178.36\n","----------------------------------------------------------------\n","Params to learn:\n","\t features.1.weight\n","\t features.1.bias\n","\t features.4.0.bn1.weight\n","\t features.4.0.bn1.bias\n","\t features.4.0.bn2.weight\n","\t features.4.0.bn2.bias\n","\t features.4.1.bn1.weight\n","\t features.4.1.bn1.bias\n","\t features.4.1.bn2.weight\n","\t features.4.1.bn2.bias\n","\t features.4.2.bn1.weight\n","\t features.4.2.bn1.bias\n","\t features.4.2.bn2.weight\n","\t features.4.2.bn2.bias\n","\t features.5.0.bn1.weight\n","\t features.5.0.bn1.bias\n","\t features.5.0.bn2.weight\n","\t features.5.0.bn2.bias\n","\t features.5.1.bn1.weight\n","\t features.5.1.bn1.bias\n","\t features.5.1.bn2.weight\n","\t features.5.1.bn2.bias\n","\t features.5.2.bn1.weight\n","\t features.5.2.bn1.bias\n","\t features.5.2.bn2.weight\n","\t features.5.2.bn2.bias\n","\t features.5.3.bn1.weight\n","\t features.5.3.bn1.bias\n","\t features.5.3.bn2.weight\n","\t features.5.3.bn2.bias\n","\t features.6.0.conv1.weight\n","\t features.6.0.bn1.weight\n","\t features.6.0.bn1.bias\n","\t features.6.0.conv2.weight\n","\t features.6.0.bn2.weight\n","\t features.6.0.bn2.bias\n","\t features.6.0.downsample.0.weight\n","\t features.6.0.downsample.1.weight\n","\t features.6.0.downsample.1.bias\n","\t features.6.1.conv1.weight\n","\t features.6.1.bn1.weight\n","\t features.6.1.bn1.bias\n","\t features.6.1.conv2.weight\n","\t features.6.1.bn2.weight\n","\t features.6.1.bn2.bias\n","\t features.6.2.conv1.weight\n","\t features.6.2.bn1.weight\n","\t features.6.2.bn1.bias\n","\t features.6.2.conv2.weight\n","\t features.6.2.bn2.weight\n","\t features.6.2.bn2.bias\n","\t features.6.3.conv1.weight\n","\t features.6.3.bn1.weight\n","\t features.6.3.bn1.bias\n","\t features.6.3.conv2.weight\n","\t features.6.3.bn2.weight\n","\t features.6.3.bn2.bias\n","\t features.6.4.conv1.weight\n","\t features.6.4.bn1.weight\n","\t features.6.4.bn1.bias\n","\t features.6.4.conv2.weight\n","\t features.6.4.bn2.weight\n","\t features.6.4.bn2.bias\n","\t features.6.5.conv1.weight\n","\t features.6.5.bn1.weight\n","\t features.6.5.bn1.bias\n","\t features.6.5.conv2.weight\n","\t features.6.5.bn2.weight\n","\t features.6.5.bn2.bias\n","\t features.7.0.conv1.weight\n","\t features.7.0.bn1.weight\n","\t features.7.0.bn1.bias\n","\t features.7.0.conv2.weight\n","\t features.7.0.bn2.weight\n","\t features.7.0.bn2.bias\n","\t features.7.0.downsample.0.weight\n","\t features.7.0.downsample.1.weight\n","\t features.7.0.downsample.1.bias\n","\t features.7.1.conv1.weight\n","\t features.7.1.bn1.weight\n","\t features.7.1.bn1.bias\n","\t features.7.1.conv2.weight\n","\t features.7.1.bn2.weight\n","\t features.7.1.bn2.bias\n","\t features.7.2.conv1.weight\n","\t features.7.2.bn1.weight\n","\t features.7.2.bn1.bias\n","\t features.7.2.conv2.weight\n","\t features.7.2.bn2.weight\n","\t features.7.2.bn2.bias\n","\t classifier.weight\n","\t classifier.bias\n"],"name":"stdout"}]}]}